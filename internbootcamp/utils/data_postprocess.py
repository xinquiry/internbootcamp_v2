"""
æ•°æ®åå¤„ç†å·¥å…·

æ”¯æŒå¯¹ evaluator è¾“å‡ºçš„ jsonl æ–‡ä»¶è¿›è¡Œçµæ´»çš„è¿‡æ»¤å’Œè½¬æ¢å¤„ç†
- å¯æ’æ‹”çš„ filter å‡½æ•°
- å¯æ’æ‹”çš„è½¬æ¢å‡½æ•°ï¼ˆæ”¯æŒä¸€å¯¹ä¸€æˆ–ä¸€å¯¹å¤šï¼‰
"""

import json
import hashlib
import jsonlines
from typing import Callable, List, Dict, Any, Optional, Union
from pathlib import Path
from collections import defaultdict


class DataPostProcessor:
    """
    æ•°æ®åå¤„ç†å™¨
    
    åŠŸèƒ½:
    1. è¯»å– evaluator è¾“å‡ºçš„ jsonl æ–‡ä»¶
    2. åº”ç”¨è¿‡æ»¤å‡½æ•°ç­›é€‰æ•°æ®
    3. åº”ç”¨è½¬æ¢å‡½æ•°è½¬æ¢æ•°æ®ï¼ˆæ”¯æŒä¸€å¯¹ä¸€æˆ–ä¸€å¯¹å¤šï¼‰
    4. è¾“å‡ºæ–°çš„ jsonl æ–‡ä»¶
    
    ç¤ºä¾‹ç”¨æ³•:
        # åˆ›å»ºå¤„ç†å™¨
        processor = DataPostProcessor()
        
        # æ³¨å†Œè¿‡æ»¤å‡½æ•°
        processor.add_filter(lambda x: x.get("success") == True)
        processor.add_filter(lambda x: x.get("score", 0) > 0.5)
        
        # æ³¨å†Œè½¬æ¢å‡½æ•°
        processor.add_transformer(extract_training_data)
        
        # æ‰§è¡Œå¤„ç†
        processor.process(
            input_path="eval_results.jsonl",
            output_path="filtered_results.jsonl"
        )
    """
    
    def __init__(self):
        """åˆå§‹åŒ–æ•°æ®å¤„ç†å™¨"""
        self.filters: List[Callable[[Dict[str, Any]], bool]] = []
        self.transformers: List[Callable[[Dict[str, Any]], Union[Dict[str, Any], List[Dict[str, Any]]]]] = []
        self.stats = defaultdict(int)
    
    def add_filter(self, filter_func: Callable[[Dict[str, Any]], bool], name: Optional[str] = None):
        """
        æ·»åŠ è¿‡æ»¤å‡½æ•°
        
        Args:
            filter_func: è¿‡æ»¤å‡½æ•°ï¼Œæ¥æ”¶ä¸€ä¸ªå­—å…¸ï¼Œè¿”å›å¸ƒå°”å€¼
                        è¿”å› True è¡¨ç¤ºä¿ç•™è¯¥æ•°æ®ï¼ŒFalse è¡¨ç¤ºè¿‡æ»¤æ‰
            name: è¿‡æ»¤å‡½æ•°çš„åç§°ï¼ˆå¯é€‰ï¼Œç”¨äºç»Ÿè®¡ï¼‰
        
        ç¤ºä¾‹:
            processor.add_filter(lambda x: x.get("success") == True)
            processor.add_filter(lambda x: x.get("score", 0) > 0.5, name="high_score")
        """
        if name:
            filter_func._filter_name = name
        self.filters.append(filter_func)
        return self
    
    def add_transformer(self, transform_func: Callable[[Dict[str, Any]], Union[Dict[str, Any], List[Dict[str, Any]]]], name: Optional[str] = None):
        """
        æ·»åŠ è½¬æ¢å‡½æ•°
        
        Args:
            transform_func: è½¬æ¢å‡½æ•°ï¼Œæ¥æ”¶ä¸€ä¸ªå­—å…¸ï¼Œè¿”å›ä¸€ä¸ªå­—å…¸æˆ–å­—å…¸åˆ—è¡¨
                           - è¿”å›å­—å…¸è¡¨ç¤ºä¸€å¯¹ä¸€è½¬æ¢
                           - è¿”å›åˆ—è¡¨è¡¨ç¤ºä¸€å¯¹å¤šè½¬æ¢
                           - è¿”å› None è¡¨ç¤ºè·³è¿‡è¯¥æ•°æ®
            name: è½¬æ¢å‡½æ•°çš„åç§°ï¼ˆå¯é€‰ï¼Œç”¨äºç»Ÿè®¡ï¼‰
        
        ç¤ºä¾‹:
            # ä¸€å¯¹ä¸€è½¬æ¢
            processor.add_transformer(lambda x: {"text": x["messages"][-1]["content"]})
            
            # ä¸€å¯¹å¤šè½¬æ¢
            def split_by_turns(data):
                return [{"turn": i, "msg": msg} for i, msg in enumerate(data["messages"])]
            processor.add_transformer(split_by_turns)
        """
        if name:
            transform_func._transform_name = name
        self.transformers.append(transform_func)
        return self
    
    def clear_filters(self):
        """æ¸…ç©ºæ‰€æœ‰è¿‡æ»¤å‡½æ•°"""
        self.filters.clear()
        return self
    
    def clear_transformers(self):
        """æ¸…ç©ºæ‰€æœ‰è½¬æ¢å‡½æ•°"""
        self.transformers.clear()
        return self
    
    def _apply_filters(self, data: Dict[str, Any]) -> bool:
        """
        åº”ç”¨æ‰€æœ‰è¿‡æ»¤å‡½æ•°
        
        Args:
            data: å¾…è¿‡æ»¤çš„æ•°æ®
        
        Returns:
            bool: True è¡¨ç¤ºé€šè¿‡æ‰€æœ‰è¿‡æ»¤å™¨ï¼ŒFalse è¡¨ç¤ºè¢«è¿‡æ»¤
        """
        for filter_func in self.filters:
            try:
                if not filter_func(data):
                    filter_name = getattr(filter_func, '_filter_name', 'unnamed')
                    self.stats[f'filtered_by_{filter_name}'] += 1
                    return False
            except Exception as e:
                print(f"âš ï¸ è¿‡æ»¤å‡½æ•°æ‰§è¡Œå‡ºé”™: {e}")
                self.stats['filter_errors'] += 1
                return False
        return True
    
    def _apply_transformers(self, data: Dict[str, Any]) -> List[Dict[str, Any]]:
        """
        åº”ç”¨æ‰€æœ‰è½¬æ¢å‡½æ•°
        
        Args:
            data: å¾…è½¬æ¢çš„æ•°æ®
        
        Returns:
            List[Dict[str, Any]]: è½¬æ¢åçš„æ•°æ®åˆ—è¡¨ï¼ˆå¯èƒ½ä¸ºç©ºï¼‰
        """
        results = [data]
        
        for transform_func in self.transformers:
            new_results = []
            for item in results:
                try:
                    transformed = transform_func(item)
                    
                    # å¤„ç†ä¸åŒçš„è¿”å›ç±»å‹
                    if transformed is None:
                        # è·³è¿‡è¯¥æ•°æ®
                        continue
                    elif isinstance(transformed, list):
                        # ä¸€å¯¹å¤šè½¬æ¢
                        new_results.extend(transformed)
                        self.stats['one_to_many_transforms'] += len(transformed) - 1
                    elif isinstance(transformed, dict):
                        # ä¸€å¯¹ä¸€è½¬æ¢
                        new_results.append(transformed)
                    else:
                        print(f"âš ï¸ è½¬æ¢å‡½æ•°è¿”å›äº†ä¸æ”¯æŒçš„ç±»å‹: {type(transformed)}")
                        self.stats['transform_type_errors'] += 1
                        
                except Exception as e:
                    print(f"âš ï¸ è½¬æ¢å‡½æ•°æ‰§è¡Œå‡ºé”™: {e}")
                    self.stats['transform_errors'] += 1
            
            results = new_results
        
        return results
    
    def process(
        self,
        input_path: Union[str, Path],
        output_path: Optional[Union[str, Path]] = None,
        verbose: bool = True
    ) -> Dict[str, int]:
        """
        å¤„ç† jsonl æ–‡ä»¶
        
        Args:
            input_path: è¾“å…¥æ–‡ä»¶è·¯å¾„
            output_path: è¾“å‡ºæ–‡ä»¶è·¯å¾„ï¼ˆå¯é€‰ï¼‰ã€‚å¦‚æœä¸æä¾›ï¼Œå°†è‡ªåŠ¨ç”Ÿæˆä¸º input_path_processed.jsonl
            verbose: æ˜¯å¦æ‰“å°è¯¦ç»†ä¿¡æ¯
        
        Returns:
            Dict[str, int]: å¤„ç†ç»Ÿè®¡ä¿¡æ¯
        """
        input_path = Path(input_path)
        
        # å¦‚æœæœªæä¾›è¾“å‡ºè·¯å¾„ï¼Œè‡ªåŠ¨ç”Ÿæˆ
        if output_path is None:
            output_path = input_path.parent / f"{input_path.stem}_processed{input_path.suffix}"
        else:
            output_path = Path(output_path)
        
        # ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
        output_path.parent.mkdir(parents=True, exist_ok=True)
        
        # é‡ç½®ç»Ÿè®¡ä¿¡æ¯
        self.stats = defaultdict(int)
        
        if verbose:
            print(f"ğŸ“– æ­£åœ¨è¯»å–è¾“å…¥æ–‡ä»¶: {input_path}")
        
        # å¤„ç†æ•°æ®
        with jsonlines.open(input_path) as reader, \
             jsonlines.open(output_path, mode='w') as writer:
            
            for line in reader:
                self.stats['total_input'] += 1
                
                # åº”ç”¨è¿‡æ»¤å™¨
                if not self._apply_filters(line):
                    self.stats['total_filtered'] += 1
                    continue
                
                # åº”ç”¨è½¬æ¢å™¨
                transformed_items = self._apply_transformers(line)
                
                # å†™å…¥ç»“æœ
                for item in transformed_items:
                    writer.write(item)
                    self.stats['total_output'] += 1
        
        if verbose:
            self._print_stats()
            print(f"âœ… å¤„ç†å®Œæˆï¼Œç»“æœå·²ä¿å­˜åˆ°: {output_path}")
        
        return dict(self.stats)
    
    def _print_stats(self):
        """æ‰“å°ç»Ÿè®¡ä¿¡æ¯"""
        print(f"\n{'='*60}")
        print(f"ğŸ“Š æ•°æ®å¤„ç†ç»Ÿè®¡")
        print(f"{'='*60}")
        print(f"è¾“å…¥æ•°æ®æ€»æ•°: {self.stats['total_input']}")
        print(f"è¿‡æ»¤æ‰çš„æ•°æ®: {self.stats['total_filtered']}")
        print(f"è¾“å‡ºæ•°æ®æ€»æ•°: {self.stats['total_output']}")
        
        if self.stats['one_to_many_transforms'] > 0:
            print(f"ä¸€å¯¹å¤šè½¬æ¢æ–°å¢: {self.stats['one_to_many_transforms']}")
        
        if self.stats['filter_errors'] > 0:
            print(f"âš ï¸ è¿‡æ»¤é”™è¯¯æ•°: {self.stats['filter_errors']}")
        
        if self.stats['transform_errors'] > 0:
            print(f"âš ï¸ è½¬æ¢é”™è¯¯æ•°: {self.stats['transform_errors']}")
        
        if self.stats['transform_type_errors'] > 0:
            print(f"âš ï¸ è½¬æ¢ç±»å‹é”™è¯¯æ•°: {self.stats['transform_type_errors']}")
        
        # æ‰“å°å„ä¸ªè¿‡æ»¤å™¨çš„ç»Ÿè®¡
        filter_stats = {k: v for k, v in self.stats.items() if k.startswith('filtered_by_')}
        if filter_stats:
            print(f"\næŒ‰è¿‡æ»¤å™¨åˆ†ç±»ç»Ÿè®¡:")
            for filter_name, count in filter_stats.items():
                print(f"  - {filter_name}: {count}")
        
        print(f"{'='*60}\n")


# ============= é¢„å®šä¹‰çš„å¸¸ç”¨è¿‡æ»¤å‡½æ•° =============

def filter_by_success(data: Dict[str, Any]) -> bool:
    """åªä¿ç•™æˆåŠŸçš„æ ·æœ¬"""
    return data.get("success", False) == True


def filter_by_score(min_score: float = 0.0, max_score: float = 1.0) -> Callable:
    """
    æŒ‰åˆ†æ•°èŒƒå›´è¿‡æ»¤
    
    Args:
        min_score: æœ€å°åˆ†æ•°ï¼ˆåŒ…å«ï¼‰
        max_score: æœ€å¤§åˆ†æ•°ï¼ˆåŒ…å«ï¼‰
    
    Returns:
        è¿‡æ»¤å‡½æ•°
    """
    def _filter(data: Dict[str, Any]) -> bool:
        score = data.get("score", 0)
        return min_score <= score <= max_score
    return _filter


def filter_by_data_source(data_source: str) -> Callable:
    """
    æŒ‰æ•°æ®æºè¿‡æ»¤
    
    Args:
        data_source: æ•°æ®æºåç§°
    
    Returns:
        è¿‡æ»¤å‡½æ•°
    """
    def _filter(data: Dict[str, Any]) -> bool:
        return data.get("input", {}).get("data_source") == data_source
    return _filter


def filter_by_field(field_path: str, expected_value: Any, default: Any = None) -> Callable:
    """
    æŒ‰å­—æ®µå€¼è¿‡æ»¤ï¼ˆæ”¯æŒåµŒå¥—å­—æ®µï¼‰
    
    Args:
        field_path: å­—æ®µè·¯å¾„ï¼Œç”¨ç‚¹å·åˆ†éš”ï¼Œä¾‹å¦‚ "input.extra_info.generator_name"
        expected_value: æœŸæœ›çš„å€¼
        default: å­—æ®µä¸å­˜åœ¨æ—¶çš„é»˜è®¤å€¼
    
    Returns:
        è¿‡æ»¤å‡½æ•°
    
    ç¤ºä¾‹:
        filter_by_field("input.extra_info.split", "test")
        filter_by_field("score", 1.0)
    """
    def _filter(data: Dict[str, Any]) -> bool:
        # è§£æåµŒå¥—å­—æ®µ
        value = data
        for key in field_path.split('.'):
            if isinstance(value, dict):
                value = value.get(key, default)
            else:
                return False
        return value == expected_value
    return _filter


# ============= é¢„å®šä¹‰çš„å¸¸ç”¨è½¬æ¢å‡½æ•° =============

def expand_messages_prefixes(data: Dict[str, Any]) -> List[Dict[str, Any]]:
    """å°†å¤šè½®å¯¹è¯æŒ‰assistantæ¶ˆæ¯å±•å¼€ä¸ºå‰ç¼€é›†"""
    result_data: List[Dict[str, Any]] = []
    full_message = data.get("messages", [])
    prompt_message = []
    input_data = data.get("input", {})
    if "prompt" in input_data and input_data["prompt"] is not None:
        prompt_message = input_data["prompt"]
    elif "messages" in input_data and input_data["messages"] is not None:
        prompt_message = input_data["messages"]
    else:
        raise ValueError("prompt or messages is not found in input")
    
    # æ‰¾åˆ°full_messageä¸­æ‰€æœ‰assistantè§’è‰²çš„æ¶ˆæ¯ç´¢å¼•
    prompt_len = len(prompt_message)
    assistant_indices = []
    for i in range(prompt_len, len(full_message)):
        if isinstance(full_message[i], dict) and full_message[i].get("role") == "assistant":
            assistant_indices.append(i)
    
    # æŒ‰æ¯ä¸ªassistantæ¶ˆæ¯æ‹†åˆ†ï¼šprompt + ç¬¬ä¸€æ¡asst, prompt + ç¬¬ä¸€æ¡asst + ä¸­é—´æ¶ˆæ¯ + ç¬¬äºŒæ¡asst, ...
    for idx in assistant_indices:
        prefix = full_message[:idx+1]  # ä»å¼€å§‹åˆ°å½“å‰assistantæ¶ˆæ¯ï¼ˆåŒ…å«ï¼‰
        temp_data = data.copy()
        temp_data["messages"] = prefix
        result_data.append(temp_data)
    
    return result_data

def extract_messages_only(data: Dict[str, Any]) -> Dict[str, Any]:
    """æå–å¯¹è¯æ¶ˆæ¯"""
    return {
        "messages": data.get("messages", []),
        "score": data.get("score", 0),
        "success": data.get("success", False)
    }


def extract_for_training(data: Dict[str, Any]) -> Dict[str, Any]:
    """æå–ç”¨äºè®­ç»ƒçš„æ•°æ®"""
    new_data = {
        "data_source": data.get("input", {}).get("data_source"),
        "prompt": data.get("input", {}).get("prompt", []),
        "messages": data.get("messages", []),
        "tools": data.get("tools", []),
    }
    new_data = generate_id_to_data(new_data)
    return new_data

def generate_id_to_data(data: Dict[str, Any]) -> Dict[str, Any]:
    """æ·»åŠ idåˆ°æ•°æ®ï¼ˆåŸºäºå†…å®¹çš„å“ˆå¸Œå€¼ç”Ÿæˆç¡®å®šæ€§idï¼‰"""
    # ä½¿ç”¨æ¶ˆæ¯å†…å®¹ç”Ÿæˆç¡®å®šæ€§ idï¼ˆç›¸åŒå†…å®¹ç”Ÿæˆç›¸åŒidï¼Œä¾¿äºå»é‡ï¼‰
    content = json.dumps(data.get("messages", []), sort_keys=True, ensure_ascii=False)
    content_hash = hashlib.md5(content.encode('utf-8')).hexdigest()
    new_data = {'id': content_hash}
    new_data.update(data)
    return new_data

def extract_assistant_responses(data: Dict[str, Any]) -> List[Dict[str, Any]]:
    """
    æå–æ‰€æœ‰ assistant çš„å“åº”ï¼ˆä¸€å¯¹å¤šè½¬æ¢ï¼‰
    
    Returns:
        æ¯ä¸ª assistant æ¶ˆæ¯å¯¹åº”ä¸€ä¸ªå­—å…¸
    """
    messages = data.get("messages", [])
    assistant_messages = [msg for msg in messages if msg.get("role") == "assistant"]
    
    return [
        {
            "index": idx,
            "content": msg.get("content"),
            "tool_calls": msg.get("tool_calls"),
            "data_source": data.get("input", {}).get("data_source"),
            "score": data.get("score", 0)
        }
        for idx, msg in enumerate(assistant_messages)
    ]


def create_field_extractor(*field_paths: str) -> Callable:
    """
    åˆ›å»ºä¸€ä¸ªæå–æŒ‡å®šå­—æ®µçš„è½¬æ¢å‡½æ•°
    
    Args:
        *field_paths: è¦æå–çš„å­—æ®µè·¯å¾„ï¼ˆæ”¯æŒåµŒå¥—ï¼Œç”¨ç‚¹å·åˆ†éš”ï¼‰
    
    Returns:
        è½¬æ¢å‡½æ•°
    
    ç¤ºä¾‹:
        # æå–å¤šä¸ªå­—æ®µ
        extractor = create_field_extractor(
            "input.data_source",
            "score",
            "messages",
            "input.extra_info.generator_name"
        )
        processor.add_transformer(extractor)
    """
    def _get_nested_value(data: Dict[str, Any], path: str, default: Any = None) -> Any:
        """è·å–åµŒå¥—å­—æ®µçš„å€¼"""
        value = data
        for key in path.split('.'):
            if isinstance(value, dict):
                value = value.get(key, default)
            else:
                return default
        return value
    
    def _extract(data: Dict[str, Any]) -> Dict[str, Any]:
        result = {}
        for path in field_paths:
            # ä½¿ç”¨è·¯å¾„çš„æœ€åä¸€éƒ¨åˆ†ä½œä¸ºé”®å
            key = path.split('.')[-1]
            result[key] = _get_nested_value(data, path)
        return result
    
    return _extract


def create_custom_transformer(transform_dict: Dict[str, Any]) -> Callable:
    """
    åˆ›å»ºä¸€ä¸ªè‡ªå®šä¹‰è½¬æ¢å‡½æ•°ï¼Œæ”¯æŒå­—æ®µé‡å‘½åå’Œé»˜è®¤å€¼
    
    Args:
        transform_dict: è½¬æ¢é…ç½®å­—å…¸
            é”®: è¾“å‡ºå­—æ®µå
            å€¼: å¯ä»¥æ˜¯:
                - str: è¾“å…¥å­—æ®µè·¯å¾„ï¼ˆæ”¯æŒåµŒå¥—ï¼‰
                - tuple: (å­—æ®µè·¯å¾„, é»˜è®¤å€¼)
                - callable: è‡ªå®šä¹‰å‡½æ•°ï¼Œæ¥æ”¶åŸå§‹æ•°æ®è¿”å›å­—æ®µå€¼
    
    Returns:
        è½¬æ¢å‡½æ•°
    
    ç¤ºä¾‹:
        transformer = create_custom_transformer({
            "text": "messages[-1].content",  # æå–æœ€åä¸€æ¡æ¶ˆæ¯çš„å†…å®¹
            "source": "input.data_source",
            "gen": ("input.extra_info.generator_name", "unknown"),
            "final_score": lambda x: x.get("score", 0) * 100,
            "success": "success"
        })
    """
    def _get_nested_value(data: Dict[str, Any], path: str, default: Any = None) -> Any:
        """è·å–åµŒå¥—å­—æ®µçš„å€¼ï¼Œæ”¯æŒåˆ—è¡¨ç´¢å¼•"""
        value = data
        for key in path.split('.'):
            if isinstance(value, dict):
                value = value.get(key, default)
            elif isinstance(value, list) and key.startswith('[') and key.endswith(']'):
                try:
                    index = int(key[1:-1])
                    value = value[index] if -len(value) <= index < len(value) else default
                except (ValueError, IndexError):
                    return default
            else:
                return default
        return value
    
    def _transform(data: Dict[str, Any]) -> Dict[str, Any]:
        result = {}
        for output_key, config in transform_dict.items():
            if callable(config):
                # è‡ªå®šä¹‰å‡½æ•°
                result[output_key] = config(data)
            elif isinstance(config, tuple):
                # (è·¯å¾„, é»˜è®¤å€¼)
                path, default = config
                result[output_key] = _get_nested_value(data, path, default)
            elif isinstance(config, str):
                # å­—æ®µè·¯å¾„
                result[output_key] = _get_nested_value(data, config)
            else:
                # ç›´æ¥ä½¿ç”¨é…ç½®å€¼
                result[output_key] = config
        return result
    
    return _transform


# ============= å‘½ä»¤è¡Œæ¥å£ =============

def main():
    """å‘½ä»¤è¡Œæ¥å£ç¤ºä¾‹"""
    import argparse
    
    parser = argparse.ArgumentParser(description="æ•°æ®åå¤„ç†å·¥å…·")
    parser.add_argument("input", help="è¾“å…¥ jsonl æ–‡ä»¶è·¯å¾„")
    parser.add_argument("output", nargs='?', default=None, help="è¾“å‡º jsonl æ–‡ä»¶è·¯å¾„ï¼ˆå¯é€‰ï¼Œä¸æä¾›åˆ™è‡ªåŠ¨ç”Ÿæˆï¼‰")
    parser.add_argument("--filter-success", action="store_true", help="åªä¿ç•™æˆåŠŸçš„æ ·æœ¬")
    parser.add_argument("--min-score", type=float, default=0.9, help="æœ€å°åˆ†æ•°")
    parser.add_argument("--max-score", type=float, default=1.0, help="æœ€å¤§åˆ†æ•°")
    parser.add_argument("--data-source", type=str, help="æŒ‰æ•°æ®æºè¿‡æ»¤")
    parser.add_argument("--extract-training", action="store_true", help="æå–è®­ç»ƒæ•°æ®æ ¼å¼")
    parser.add_argument("--extract-messages", action="store_true", help="åªæå–æ¶ˆæ¯")
    parser.add_argument("--expand-messages-prefixes", action="store_true", help="å°†å¤šè½®å¯¹è¯å±•å¼€ä¸ºå‰ç¼€é›†")
    
    args = parser.parse_args()
    
    # åˆ›å»ºå¤„ç†å™¨
    processor = DataPostProcessor()
    
    # æ·»åŠ è¿‡æ»¤å™¨
    if args.filter_success:
        processor.add_filter(filter_by_success, name="success")
    
    if args.min_score > 0.0 or args.max_score < 1.0:
        processor.add_filter(filter_by_score(args.min_score, args.max_score), name="score")
    
    if args.data_source:
        processor.add_filter(filter_by_data_source(args.data_source), name="data_source")
    
    # æ·»åŠ è½¬æ¢å™¨(éœ€è¦æŒ‰é¡ºåºæ·»åŠ ï¼Œå› ä¸ºæœ‰äº›è½¬æ¢å‡½æ•°ä¼šä¿®æ”¹æ•°æ®)
    if args.expand_messages_prefixes:
        processor.add_transformer(expand_messages_prefixes, name="expand_messages_prefixes")
    if args.extract_training:
        processor.add_transformer(extract_for_training, name="training_format")
    if args.extract_messages:
        processor.add_transformer(extract_messages_only, name="messages_only")

    
    # æ‰§è¡Œå¤„ç†
    processor.process(args.input, args.output)


if __name__ == "__main__":
    main()


"""
ç¤ºä¾‹ç”¨æ³•:

# æ–¹å¼1: ä¸æŒ‡å®šè¾“å‡ºè·¯å¾„ï¼Œè‡ªåŠ¨ç”Ÿæˆ (ä¾‹å¦‚: eval_results_20251027171406_processed.jsonl)
python Bootcampv2/utils/data_postprocess.py \
    Bootcampv2/example_bootcamp/data/eval_output/qwen3-30b-a3b-thinking-2507/eval_results_20251031165611.jsonl \
    --expand-messages-prefixes \
    --extract-training \
    --min-score 0.9 \
    --max-score 1.0

# æ–¹å¼2: æŒ‡å®šè¾“å‡ºè·¯å¾„
python Bootcampv2/utils/data_postprocess.py \
    Bootcampv2/example_bootcamp/data/eval_output/deepseekv3-1-terminus/eval_results_20251027171406.jsonl \
    Bootcampv2/example_bootcamp/data/eval_output/deepseekv3-1-terminus/eval_results_custom.jsonl \
    --expand-messages-prefixes \
    --min-score 0.9 \
    --max-score 1.0
"""